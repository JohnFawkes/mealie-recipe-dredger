"""
Recipe Dredger - Enhanced Public Edition
Intelligent recipe scraper for Mealie/Tandoor with optimized performance and reliability.
Usage: python3 dredger.py [--dry-run] [--limit 10] [--sites my_sites.json] [--no-cache] [--version]
"""

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
from langdetect import detect, DetectorFactory
import json
import time
import os
import random
import re
import logging
import sys
import argparse
import signal
from urllib.parse import urlparse
from dotenv import load_dotenv
from typing import Optional, Set, List, Dict, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta

# --- CONSTANTS ---
VERSION = "1.0.0-beta.11"

# --- OPTIONAL VISUALS ---
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False

# --- LOAD ENV VARS ---
load_dotenv()

# --- LOGGING CONFIGURATION ---
LOG_LEVEL = os.getenv('LOG_LEVEL', 'INFO').upper()
logging.basicConfig(
    level=LOG_LEVEL,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger("dredger")

# --- CONFIGURATION DEFAULTS ---
DEFAULT_TARGET = int(os.getenv('TARGET_RECIPES_PER_SITE', 50))
DEFAULT_DEPTH = int(os.getenv('SCAN_DEPTH', 1000))
DRY_RUN = os.getenv('DRY_RUN', 'true').lower() == 'true'

# --- PLATFORM SETTINGS ---
MEALIE_ENABLED = os.getenv('MEALIE_ENABLED', 'true').lower() == 'true'
MEALIE_URL = os.getenv('MEALIE_URL', 'http://localhost:9000').rstrip('/')
MEALIE_API_TOKEN = os.getenv('MEALIE_API_TOKEN', 'your-token')

TANDOOR_ENABLED = os.getenv('TANDOOR_ENABLED', 'false').lower() == 'true'
TANDOOR_URL = os.getenv('TANDOOR_URL', 'http://localhost:8080').rstrip('/')
TANDOOR_API_KEY = os.getenv('TANDOOR_API_KEY', 'your-key')

# Rate limiting
DEFAULT_CRAWL_DELAY = float(os.getenv('CRAWL_DELAY', 2.0))
RESPECT_ROBOTS_TXT = os.getenv('RESPECT_ROBOTS_TXT', 'true').lower() == 'true'

# Notifications
NOTIFICATION_WEBHOOK_URL = os.getenv('NOTIFICATION_WEBHOOK_URL', '').strip()

# Library sync
SYNC_LIBRARY = os.getenv('SYNC_LIBRARY', 'true').lower() == 'true'

# Language filtering (e.g., 'en', 'es', 'fr' or empty to allow all)
LANGUAGE_FILTER = os.getenv('LANGUAGE_FILTER', '').strip().lower()

# Seed langdetect for reproducible results
DetectorFactory.seed = 0

# Memory settings
os.makedirs("data", exist_ok=True)
REJECT_FILE = "data/rejects.json"
IMPORTED_FILE = "data/imported.json"
RETRY_FILE = "data/retry_queue.json"
STATS_FILE = "data/stats.json"
SITEMAP_CACHE_FILE = "data/sitemap_cache.json"
CACHE_EXPIRY_DAYS = int(os.getenv('CACHE_EXPIRY_DAYS', 7))

# --- TUNING CONSTANTS ---
MAX_SUB_SITEMAPS = 3
FLUSH_THRESHOLD = 50
SITEMAP_TIMEOUT = 10
IMPORT_TIMEOUT = 20
ROBOTS_TXT_TIMEOUT = 5
MAX_SITEMAP_DEPTH = 2
KNOWN_RECIPE_CLASSES = ['wp-recipe-maker', 'tasty-recipes', 'mv-create-card', 'recipe-card']



# --- FALLBACK LIST (10 Major Sites) ---
# Note: The full categorized list (120+ sites) is in sites.json
DEFAULT_SITES = [
    "https://www.seriouseats.com",
    "https://www.bonappetit.com",
    "https://www.recipetineats.com",
    "https://smittenkitchen.com",
    "https://minimalistbaker.com",
    "https://www.justonecookbook.com",
    "https://www.woksoflife.com",
    "https://sallysbakingaddiction.com",
    "https://www.skinnytaste.com",
    "https://www.budgetbytes.com"
]

# --- PARANOID FILTERS ---
LISTICLE_REGEX = re.compile(r'(\d+)-(best|top|must|favorite|easy|healthy|quick|ways|things)', re.IGNORECASE)
BAD_KEYWORDS = ["roundup", "collection", "guide", "review", "giveaway", "shop", "store", "product"]

# ============================================================================
# DATA MODELS
# ============================================================================

@dataclass
class RecipeCandidate:
    url: str
    priority: int = 0 
    
    def __hash__(self):
        return hash(self.url)
    
    def __eq__(self, other):
        return self.url == other.url if isinstance(other, RecipeCandidate) else self.url == other

@dataclass
class SiteStats:
    site_url: str
    recipes_found: int = 0
    recipes_imported: int = 0
    recipes_rejected: int = 0
    errors: int = 0
    last_run: Optional[str] = None
    
    def to_dict(self) -> dict:
        return asdict(self)

# ============================================================================
# PERSISTENT STORAGE MANAGER
# ============================================================================

class StorageManager:
    def __init__(self):
        self.rejects: Set[str] = self._load_json_set(REJECT_FILE)
        self.imported: Set[str] = self._load_json_set(IMPORTED_FILE)
        self.retry_queue: Dict[str, dict] = self._load_json_dict(RETRY_FILE)
        self.stats: Dict[str, dict] = self._load_json_dict(STATS_FILE)
        self.sitemap_cache: Dict[str, dict] = self._load_json_dict(SITEMAP_CACHE_FILE)
        
        self._changes_since_flush = 0
        self._flush_threshold = FLUSH_THRESHOLD
        
    def _load_json_set(self, filename: str) -> Set[str]:
        if os.path.exists(filename):
            try:
                with open(filename, 'r') as f:
                    return set(json.load(f))
            except Exception as e:
                logger.warning(f"Error loading {filename}: {e}")
        return set()
    
    def _load_json_dict(self, filename: str) -> dict:
        if os.path.exists(filename):
            try:
                with open(filename, 'r') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"Error loading {filename}: {e}")
        return {}
    
    def _save_json_set(self, filename: str, data_set: Set[str]):
        with open(filename, 'w') as f:
            json.dump(list(data_set), f, indent=2)
    
    def _save_json_dict(self, filename: str, data_dict: dict):
        with open(filename, 'w') as f:
            json.dump(data_dict, f, indent=2)
    
    def add_imported(self, url: str):
        self.imported.add(url)
        self._changes_since_flush += 1
        self._auto_flush()
    
    def add_reject(self, url: str):
        self.rejects.add(url)
        self._changes_since_flush += 1
        self._auto_flush()
    
    def add_retry(self, url: str, reason: str):
        self.retry_queue[url] = {
            'reason': reason,
            'attempts': 0,
            'last_attempt': datetime.now().isoformat()
        }
        self._changes_since_flush += 1
        self._auto_flush()
    
    def update_stats(self, site_url: str, stats: SiteStats):
        self.stats[site_url] = stats.to_dict()
        self._changes_since_flush += 1
        self._auto_flush()
    
    def get_cached_sitemap(self, site_url: str) -> Optional[dict]:
        if site_url not in self.sitemap_cache: 
            return None
        
        cache_entry = self.sitemap_cache[site_url]
        cached_time = datetime.fromisoformat(cache_entry['timestamp'])
        
        if datetime.now() - cached_time > timedelta(days=CACHE_EXPIRY_DAYS):
            return None
        
        return cache_entry
    
    def cache_sitemap(self, site_url: str, sitemap_url: str, urls: List[str]):
        self.sitemap_cache[site_url] = {
            'sitemap_url': sitemap_url,
            'urls': urls,
            'timestamp': datetime.now().isoformat()
        }
        self._changes_since_flush += 1
        self._auto_flush()
    
    def _auto_flush(self):
        if self._changes_since_flush >= self._flush_threshold:
            self.flush_all()
    
    def flush_all(self):
        self._save_json_set(REJECT_FILE, self.rejects)
        self._save_json_set(IMPORTED_FILE, self.imported)
        self._save_json_dict(RETRY_FILE, self.retry_queue)
        self._save_json_dict(STATS_FILE, self.stats)
        self._save_json_dict(SITEMAP_CACHE_FILE, self.sitemap_cache)
        self._changes_since_flush = 0

# ============================================================================
# GRACEFUL KILLER
# ============================================================================

class GracefulKiller:
    """Catches Docker stop signals to allow safe shutdown."""
    def __init__(self):
        self.kill_now = False
        signal.signal(signal.SIGINT, self.exit_gracefully)
        signal.signal(signal.SIGTERM, self.exit_gracefully)

    def exit_gracefully(self, signum, frame):
        signal_name = 'SIGINT (Ctrl+C)' if signum == signal.SIGINT else 'SIGTERM (Docker Stop)'
        logger.info(f"üõë Received {signal_name}. Initiating graceful shutdown...")
        self.kill_now = True

# ============================================================================
# RATE LIMITER (FIXED: Respects HTTP vs HTTPS scheme)
# ============================================================================
class RateLimiter:
    def __init__(self):
        self.last_request: Dict[str, float] = {}
        self.crawl_delays: Dict[str, float] = {}
        self.session = get_session()
    
    def get_domain(self, url: str) -> str:
        return urlparse(url).netloc
    
    def get_crawl_delay(self, url: str) -> float:
        domain = self.get_domain(url)
        if domain in self.crawl_delays:
            return self.crawl_delays[domain]
        
        delay = DEFAULT_CRAWL_DELAY
        if RESPECT_ROBOTS_TXT:
            try:
                # QC FIX: Use the actual scheme (http or https) from the URL
                parsed = urlparse(url)
                scheme = parsed.scheme if parsed.scheme else "https"
                
                # If checking localhost or LAN IP, use HTTP unless specified otherwise
                if "192.168" in domain or "127.0.0.1" in domain or "localhost" in domain:
                    if scheme == "https": scheme = "http" # Fallback to HTTP for LAN if unsure

                r = self.session.get(f"{scheme}://{domain}/robots.txt", timeout=ROBOTS_TXT_TIMEOUT)
                if r.status_code == 200:
                    for line in r.text.splitlines():
                        if line.lower().startswith('crawl-delay:'):
                            try:
                                delay = float(line.split(':')[1].strip())
                                break
                            except ValueError: 
                                pass
            except Exception: 
                pass
        
        self.crawl_delays[domain] = delay
        return delay
    
    def wait_if_needed(self, url: str):
        domain = self.get_domain(url)
        delay = self.get_crawl_delay(url) # Pass full URL so we know the scheme
        
        if domain in self.last_request:
            elapsed = time.time() - self.last_request[domain]
            if elapsed < delay:
                # Add jitter (0.5x to 1.5x) to mimic human variance
                jitter = random.uniform(0.5, 1.5)
                sleep_time = (delay - elapsed) * jitter
                time.sleep(sleep_time)
        
        self.last_request[domain] = time.time()

# ============================================================================
# SESSION MANAGEMENT
# ============================================================================

def get_session() -> requests.Session:
    session = requests.Session()
    retries = Retry(
        total=3, 
        backoff_factor=1, 
        status_forcelist=[500, 502, 503, 504, 429],
        allowed_methods=["HEAD", "GET", "POST"]
    )
    session.mount('http://', HTTPAdapter(max_retries=retries))
    session.mount('https://', HTTPAdapter(max_retries=retries))
    session.headers.update({
        'User-Agent': f'Mozilla/5.0 (Windows NT 10.0; Win64; x64) RecipeDredger/{VERSION}'
    })
    return session

# ============================================================================
# SITEMAP CRAWLER (WITH GARBAGE FILTER)
# ============================================================================

class SitemapCrawler:
    def __init__(self, session: requests.Session, storage: StorageManager):
        self.session = session
        self.storage = storage
    
    def find_sitemap(self, base_url: str) -> Optional[str]:
        # 1. Check robots.txt
        try:
            r = self.session.get(f"{base_url}/robots.txt", timeout=ROBOTS_TXT_TIMEOUT)
            if r.status_code == 200:
                for line in r.text.splitlines():
                    if "Sitemap:" in line:
                        return line.split("Sitemap:")[1].strip()
        except Exception as e: 
            logger.debug(f"robots.txt fetch failed for {base_url}: {e}")
        
        # 2. Check standard candidates
        candidates = [
            f"{base_url}/sitemap_index.xml", 
            f"{base_url}/sitemap.xml",
            f"{base_url}/wp-sitemap.xml", 
            f"{base_url}/post-sitemap.xml",
            f"{base_url}/recipe-sitemap.xml"
        ]
        
        for url in candidates:
            try:
                r = self.session.head(url, timeout=ROBOTS_TXT_TIMEOUT)
                if r.status_code == 200: 
                    return url
            except Exception as e: 
                logger.debug(f"Sitemap candidate check failed for {url}: {e}")
        
        return None
    
    def fetch_sitemap_urls(self, url: str, depth: int = 0) -> List[str]:
        if depth > MAX_SITEMAP_DEPTH: 
            return []
        
        try:
            r = self.session.get(url, timeout=SITEMAP_TIMEOUT)
            if r.status_code != 200: 
                return []
            
            # Use BeautifulSoup XML parser for robust parsing
            soup = BeautifulSoup(r.content, 'xml')
            all_urls = []

            # Handle Index Sitemaps (nested)
            if soup.find('sitemap'):
                sub_maps = [loc.text for loc in soup.find_all('loc')]
                targets = [s for s in sub_maps if 'post' in s or 'recipe' in s]
                if not targets: 
                    targets = sub_maps
                
                for sub in targets[:MAX_SUB_SITEMAPS]: 
                    all_urls.extend(self.fetch_sitemap_urls(sub, depth + 1))
                return all_urls
            
            # Handle URL Sitemaps
            if soup.find('url'):
                 raw_urls = [loc.text for loc in soup.find_all('loc')]
                 
                 # --- THE GARBAGE FILTER ---
                 clean_urls = []
                 # Define extensions to skip immediately
                 junk_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.webp', '.pdf', '.zip'}
                 
                 for u in raw_urls:
                     u_lower = u.lower()
                     # 1. Drop image/binary files immediately
                     if any(u_lower.endswith(ext) for ext in junk_extensions):
                         continue
                     # 2. Drop obvious non-recipe pages
                     if any(x in u_lower for x in ['/privacy-policy', '/contact', '/about', '/login', '/wp-content/', '/cdn-cgi/']):
                         continue
                     clean_urls.append(u)
                 
                 return clean_urls
            
            return []

        except Exception as e:
            logger.warning(f"Sitemap parse error {url}: {e}")
            return []
    
    def get_urls_for_site(self, site_url: str, force_refresh: bool = False) -> List[RecipeCandidate]:
        if not force_refresh:
            cached = self.storage.get_cached_sitemap(site_url)
            if cached:
                return [RecipeCandidate(url=u) for u in cached['urls']]
        
        sitemap_url = self.find_sitemap(site_url)
        if not sitemap_url: 
            return []
        
        urls = self.fetch_sitemap_urls(sitemap_url)
        self.storage.cache_sitemap(site_url, sitemap_url, urls)
        return [RecipeCandidate(url=u) for u in urls]

# ============================================================================
# RECIPE VERIFIER
# ============================================================================
class RecipeVerifier:
    def __init__(self, session: requests.Session):
        self.session = session
    
    def is_paranoid_skip(self, url: str, soup: Optional[BeautifulSoup] = None) -> Optional[str]:
        try:
            path = urlparse(url).path
            slug = path.strip("/").split("/")[-1].lower()
            
            if LISTICLE_REGEX.search(slug): 
                return f"Listicle detected: {slug}"
            
            for kw in BAD_KEYWORDS:
                if kw in slug: 
                    return f"Bad keyword: {kw}"
            
            if soup:
                title = soup.title.string.lower() if soup.title else ""
                if "best recipes" in title or "top 10" in title: 
                    return "Listicle title"
        
        except Exception as e: 
            logger.debug(f"Paranoid skip check error for {url}: {e}")
        
        return None
    
    def verify_recipe(self, url: str) -> Tuple[bool, Optional[BeautifulSoup], Optional[str]]:
        try:
            r = self.session.get(url, timeout=SITEMAP_TIMEOUT)
            if r.status_code != 200: 
                return False, None, f"HTTP {r.status_code}"
            
            # Recipe detection
            is_recipe = False
            soup = None
            
            if '"@type":"Recipe"' in r.text or '"@type": "Recipe"' in r.text:
                is_recipe = True
            
            if not is_recipe:
                soup = BeautifulSoup(r.content, 'lxml')
                if soup.find(class_=lambda x: x and any(cls in x for cls in KNOWN_RECIPE_CLASSES)):
                    is_recipe = True
            
            if not is_recipe: 
                return False, soup, "No recipe detected"
            
            if soup is None: 
                soup = BeautifulSoup(r.content, 'lxml')
            
            # Paranoid checks
            skip_reason = self.is_paranoid_skip(url, soup)
            if skip_reason: 
                return False, soup, skip_reason
            
            # Language filter
            if LANGUAGE_FILTER:
                try:
                    text = soup.get_text(separator=' ', strip=True)[:1000]
                    if len(text) > 50:  # Need enough text for reliable detection
                        detected_lang = detect(text)
                        if detected_lang != LANGUAGE_FILTER:
                            return False, soup, f"Language mismatch: detected '{detected_lang}', want '{LANGUAGE_FILTER}'"
                except Exception as e:
                    logger.debug(f"Language detection failed for {url}: {e}")
            
            return True, soup, None
        
        except Exception as e:
            return False, None, f"Exception: {str(e)}"

# ============================================================================
# IMPORT MANAGER (AUTO-DETECT MEALIE VERSION)
# ============================================================================

class ImportManager:
    def __init__(self, session: requests.Session, storage: StorageManager, rate_limiter: RateLimiter, dry_run: bool):
        self.session = session
        self.storage = storage
        self.rate_limiter = rate_limiter
        self.dry_run = dry_run
        # Cache the working endpoint so we don't guess every time
        self.working_endpoint = None

    def import_to_mealie(self, url: str) -> Tuple[bool, Optional[str]]:
        if self.dry_run:
            logger.info(f"   [DRY RUN] Would import to Mealie: {url}")
            return True, None
        
        headers = {"Authorization": f"Bearer {MEALIE_API_TOKEN}"}
        
        # 1. Determine endpoints to try
        # If we already found the right one, use it. Otherwise, try New (v2/v3) then Old (v1)
        if self.working_endpoint:
            endpoints = [self.working_endpoint]
        else:
            endpoints = ["/api/recipes/create/url", "/api/recipes/create-url"]

        self.rate_limiter.wait_if_needed(MEALIE_URL)
        
        last_error = None

        for endpoint in endpoints:
            try:
                full_url = f"{MEALIE_URL}{endpoint}"
                r = self.session.post(full_url, headers=headers, json={"url": url}, timeout=IMPORT_TIMEOUT)
                
                # If 404/405, the endpoint is wrong/deprecated. Try the next one.
                if r.status_code in [404, 405]:
                    last_error = f"HTTP {r.status_code} on {endpoint}"
                    continue

                # Success! Save the working endpoint for future requests.
                if self.working_endpoint is None:
                    self.working_endpoint = endpoint
                    logger.debug(f"   üéØ Auto-Detected Mealie API: {endpoint}")

                if r.status_code in [200, 201]:
                    logger.info(f"   ‚úÖ [Mealie] Imported: {url}")
                    return True, None
                elif r.status_code == 409:
                    logger.info(f"   ‚ö†Ô∏è [Mealie] Duplicate: {url}")
                    return True, None
                else:
                    return False, f"HTTP {r.status_code}"
            
            except Exception as e:
                last_error = str(e)
                continue
        
        return False, f"All API attempts failed. Last error: {last_error}"

    def import_to_tandoor(self, url: str) -> Tuple[bool, Optional[str]]:
        if self.dry_run:
            logger.info(f"   [DRY RUN] Would import to Tandoor: {url}")
            return True, None
        
        headers = {"Authorization": f"Bearer {TANDOOR_API_KEY}"}
        try:
            self.rate_limiter.wait_if_needed(TANDOOR_URL)
            r = self.session.post(
                f"{TANDOOR_URL}/api/recipe/import-url/", 
                headers=headers, 
                json={"url": url}, 
                timeout=IMPORT_TIMEOUT
            )
            
            if r.status_code in [200, 201]:
                logger.info(f"   ‚úÖ [Tandoor] Imported: {url}")
                return True, None
            else:
                return False, f"HTTP {r.status_code}"
        
        except Exception as e:
            return False, str(e)
    
    def import_recipe(self, url: str) -> bool:
        success = False
        
        if MEALIE_ENABLED:
            m_ok, _ = self.import_to_mealie(url)
            success = success or m_ok
        
        if TANDOOR_ENABLED:
            t_ok, _ = self.import_to_tandoor(url)
            success = success or t_ok
            
        return success

# ============================================================================
# CLI & MAIN
# ============================================================================

def validate_config():
    """Check for common misconfigurations."""
    issues = []
    
    if not MEALIE_ENABLED and not TANDOOR_ENABLED and not DRY_RUN:
        issues.append("‚ö†Ô∏è  Warning: Both Mealie and Tandoor are disabled. Nothing will be imported!")
    
    if MEALIE_ENABLED and MEALIE_API_TOKEN == 'your-token':
        issues.append("‚ö†Ô∏è  Warning: MEALIE_API_TOKEN not configured (still set to default)")
        
    if TANDOOR_ENABLED and TANDOOR_API_KEY == 'your-key':
        issues.append("‚ö†Ô∏è  Warning: TANDOOR_API_KEY not configured (still set to default)")
        
    for issue in issues:
        logger.warning(issue)

def check_connectivity(session: requests.Session):
    """Verify API connectivity before starting the crawl. Exit early on failure."""
    if MEALIE_ENABLED and MEALIE_API_TOKEN != 'your-token':
        try:
            headers = {"Authorization": f"Bearer {MEALIE_API_TOKEN}"}
            r = session.get(f"{MEALIE_URL}/api/recipes?page=1&perPage=1", headers=headers, timeout=ROBOTS_TXT_TIMEOUT)
            if r.status_code == 200:
                logger.info(f"   ‚úÖ Mealie connectivity OK ({MEALIE_URL})")
            elif r.status_code == 401:
                logger.critical(f"‚ùå Mealie API token is invalid (HTTP 401). Check MEALIE_API_TOKEN.")
                sys.exit(1)
            else:
                logger.warning(f"‚ö†Ô∏è  Mealie returned HTTP {r.status_code} ‚Äî proceeding anyway")
        except Exception as e:
            logger.critical(f"‚ùå Cannot reach Mealie at {MEALIE_URL}: {e}")
            sys.exit(1)
    
    if TANDOOR_ENABLED and TANDOOR_API_KEY != 'your-key':
        try:
            headers = {"Authorization": f"Bearer {TANDOOR_API_KEY}"}
            r = session.get(f"{TANDOOR_URL}/api/recipe/?page=1&limit=1", headers=headers, timeout=ROBOTS_TXT_TIMEOUT)
            if r.status_code == 200:
                logger.info(f"   ‚úÖ Tandoor connectivity OK ({TANDOOR_URL})")
            elif r.status_code in [401, 403]:
                logger.critical(f"‚ùå Tandoor API key is invalid (HTTP {r.status_code}). Check TANDOOR_API_KEY.")
                sys.exit(1)
            else:
                logger.warning(f"‚ö†Ô∏è  Tandoor returned HTTP {r.status_code} ‚Äî proceeding anyway")
        except Exception as e:
            logger.critical(f"‚ùå Cannot reach Tandoor at {TANDOOR_URL}: {e}")
            sys.exit(1)

def sync_existing_library(session: requests.Session, storage: StorageManager):
    """Fetch URLs already in Mealie/Tandoor to avoid duplicate import attempts."""
    synced = 0
    
    if MEALIE_ENABLED and MEALIE_API_TOKEN != 'your-token':
        headers = {"Authorization": f"Bearer {MEALIE_API_TOKEN}"}
        page = 1
        while True:
            try:
                r = session.get(f"{MEALIE_URL}/api/recipes?page={page}&perPage=100", headers=headers, timeout=SITEMAP_TIMEOUT)
                if r.status_code != 200:
                    break
                items = r.json().get('items', [])
                if not items:
                    break
                for item in items:
                    url = item.get('orgURL') or item.get('originalURL', '')
                    if url and url.startswith('http'):
                        storage.imported.add(url)
                        synced += 1
                page += 1
            except Exception as e:
                logger.warning(f"Library sync stopped (page {page}): {e}")
                break
    
    if synced > 0:
        logger.info(f"   üìö Synced {synced} existing library URLs")

def process_retry_queue(storage: StorageManager, importer, verifier: 'RecipeVerifier', rate_limiter: 'RateLimiter') -> int:
    """Process pending retries from previous runs. Returns count of successful imports."""
    if not storage.retry_queue:
        return 0
    
    MAX_RETRY_ATTEMPTS = 3
    MIN_RETRY_INTERVAL_HOURS = 1
    imported_count = 0
    completed_urls = []
    
    eligible = []
    for url, info in storage.retry_queue.items():
        if info.get('attempts', 0) >= MAX_RETRY_ATTEMPTS:
            storage.add_reject(url)
            completed_urls.append(url)
            continue
        
        last_attempt = info.get('last_attempt', '')
        if last_attempt:
            try:
                last_time = datetime.fromisoformat(last_attempt)
                if datetime.now() - last_time < timedelta(hours=MIN_RETRY_INTERVAL_HOURS):
                    continue
            except (ValueError, TypeError):
                pass
        
        eligible.append(url)
    
    if eligible:
        logger.info(f"üîÑ Processing {len(eligible)} retries from previous runs...")
    
    for url in eligible:
        rate_limiter.wait_if_needed(url)
        is_recipe, soup, error = verifier.verify_recipe(url)
        
        if is_recipe:
            if importer.import_recipe(url):
                storage.add_imported(url)
                imported_count += 1
                completed_urls.append(url)
            else:
                storage.retry_queue[url]['attempts'] = storage.retry_queue[url].get('attempts', 0) + 1
                storage.retry_queue[url]['last_attempt'] = datetime.now().isoformat()
        else:
            storage.add_reject(url)
            completed_urls.append(url)
    
    for url in completed_urls:
        storage.retry_queue.pop(url, None)
    
    if imported_count > 0:
        logger.info(f"   ‚úÖ Retries: {imported_count} imported, {len(completed_urls) - imported_count} permanently rejected")
    
    return imported_count

def send_notification(storage: StorageManager):
    """Send a summary notification via webhook (Discord, Slack, ntfy, etc.)."""
    if not NOTIFICATION_WEBHOOK_URL:
        return
    
    summary = (
        f"üç≤ Recipe Dredger Complete ({VERSION})\n"
        f"   Imported: {len(storage.imported)}\n"
        f"   Rejected: {len(storage.rejects)}\n"
        f"   Retry Queue: {len(storage.retry_queue)}\n"
        f"   Cached Sitemaps: {len(storage.sitemap_cache)}"
    )
    
    try:
        requests.post(NOTIFICATION_WEBHOOK_URL, json={"content": summary, "text": summary}, timeout=ROBOTS_TXT_TIMEOUT)
        logger.info("üì® Notification sent")
    except Exception as e:
        logger.warning(f"Failed to send notification: {e}")

def print_summary(storage: StorageManager):
    logger.info("=" * 50)
    logger.info("üìä Session Summary:")
    logger.info(f"   Total Imported: {len(storage.imported)}")
    logger.info(f"   Total Rejected: {len(storage.rejects)}")
    logger.info(f"   In Retry Queue: {len(storage.retry_queue)}")
    logger.info(f"   Cached Sitemaps: {len(storage.sitemap_cache)}")
    logger.info("=" * 50)

def load_sites_from_source(source_path: str = None) -> List[str]:
    """Load sites from various sources with priority: CLI > local file > env > defaults"""
    
    def parse_sites_json(data) -> List[str]:
        """Parse sites from JSON, handling both array and object formats."""
        if isinstance(data, list):
            # Simple array format: ["url1", "url2", ...]
            return [s for s in data if isinstance(s, str) and s.startswith('http')]
        elif isinstance(data, dict) and 'sites' in data:
            # Object format: {"sites": ["url1", "url2", ...]}
            sites = data['sites']
            return [s for s in sites if isinstance(s, str) and s.startswith('http')]
        else:
            logger.error("Invalid sites.json format. Expected array or object with 'sites' key.")
            return []
    
    # 1. CLI Argument
    if source_path:
        if os.path.exists(source_path):
            try:
                with open(source_path, 'r') as f: 
                    data = json.load(f)
                    return parse_sites_json(data)
            except Exception as e: 
                logger.error(f"Failed to load CLI sites file: {e}")
                sys.exit(1)
        else:
            logger.error(f"File not found: {source_path}")
            sys.exit(1)

    # 2. Local sites.json
    if os.path.exists('sites.json'):
        try:
            with open('sites.json', 'r') as f: 
                data = json.load(f)
                return parse_sites_json(data)
        except Exception as e:
            logger.warning(f"Failed to load sites.json: {e}")
            pass

    # 3. Environment variable
    if os.getenv('SITES'):
        return [s.strip() for s in os.getenv('SITES').split(',') if s.strip()]

    # 4. Defaults (The Full Curated List)
    return DEFAULT_SITES

def main():
    parser = argparse.ArgumentParser(description="Recipe Dredger: Intelligent Scraper")
    parser.add_argument("--dry-run", action="store_true", help="Scan without importing")
    parser.add_argument("--limit", type=int, default=DEFAULT_TARGET, help="Recipes to import per site")
    parser.add_argument("--depth", type=int, default=DEFAULT_DEPTH, help="URLs to scan per site")
    parser.add_argument("--sites", type=str, help="Path to JSON file containing site URLs")
    parser.add_argument("--no-cache", action="store_true", help="Force fresh crawl (ignore sitemap cache)")
    parser.add_argument("--version", action="version", version=f"Recipe Dredger {VERSION}")
    args = parser.parse_args()

    # Configuration Override
    DRY_RUN_MODE = args.dry_run or DRY_RUN
    TARGET_COUNT = args.limit
    SCAN_DEPTH_COUNT = args.depth
    FORCE_REFRESH = args.no_cache
    
    # Startup checks
    session = get_session()
    validate_config()
    check_connectivity(session)
    
    sites_list = load_sites_from_source(args.sites)

    logger.info(f"üç≤ Recipe Dredger Started ({VERSION})")
    logger.info(f"   Mode: {'DRY RUN' if DRY_RUN_MODE else 'LIVE IMPORT'}")
    logger.info(f"   Targets: {len(sites_list)} sites")
    logger.info(f"   Limit: {TARGET_COUNT} per site")
    
    # Initialize components
    storage = StorageManager()
    killer = GracefulKiller()
    rate_limiter = RateLimiter()
    crawler = SitemapCrawler(session, storage)
    verifier = RecipeVerifier(session)
    importer = ImportManager(session, storage, rate_limiter, DRY_RUN_MODE)
    
    # Sync existing library to avoid duplicate API calls
    if SYNC_LIBRARY and not DRY_RUN_MODE:
        sync_existing_library(session, storage)
    
    # Process retry queue from previous runs
    process_retry_queue(storage, importer, verifier, rate_limiter)
    
    # Process sites
    random.shuffle(sites_list)
    
    # Visual Progress Bar (optional tqdm)
    iterator = sites_list
    if TQDM_AVAILABLE and len(sites_list) > 1:
        iterator = tqdm(sites_list, desc="Processing Sites", unit="site")

    for site in iterator:
        # Check for graceful shutdown
        if killer.kill_now: 
            break
        
        if not TQDM_AVAILABLE: 
            logger.info(f"üåç Processing Site: {site}")
        
        site_stats = {'imported': 0, 'rejected': 0, 'errors': 0}
        
        raw_candidates = crawler.get_urls_for_site(site, force_refresh=FORCE_REFRESH)
        if not raw_candidates: 
            continue
        
        candidates = raw_candidates[:SCAN_DEPTH_COUNT]
        random.shuffle(candidates)
        
        imported_count = 0
        for candidate in candidates:
            # Check for graceful shutdown in inner loop
            if killer.kill_now: 
                break
            
            if imported_count >= TARGET_COUNT: 
                break
            
            url = candidate.url
            
            if url in storage.imported or url in storage.rejects: 
                continue
            
            rate_limiter.wait_if_needed(url)
            
            is_recipe, soup, error = verifier.verify_recipe(url)
            
            if is_recipe:
                if importer.import_recipe(url):
                    storage.add_imported(url)
                    imported_count += 1
                    site_stats['imported'] += 1
                else:
                    site_stats['errors'] += 1
                    if not TQDM_AVAILABLE: 
                        logger.error(f"   ‚ùå Import failed: {url}")
            else:
                if not TQDM_AVAILABLE: 
                    logger.debug(f"   Skipping ({error}): {url}")
                storage.add_reject(url)
                site_stats['rejected'] += 1
        
        if not TQDM_AVAILABLE:
            logger.info(f"   Site Results: {site_stats['imported']} imported, "
                        f"{site_stats['rejected']} rejected, {site_stats['errors']} errors")
        
        storage.flush_all()
    
    # Print summary if not interrupted
    if not killer.kill_now:
        print_summary(storage)
    else:
        logger.info("‚è∏Ô∏è  Gracefully stopped by signal")
    
    # Send notification webhook
    send_notification(storage)
        
    logger.info("üèÅ Dredge Cycle Complete")

if __name__ == "__main__":
    main()

